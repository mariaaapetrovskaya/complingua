{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariaaapetrovskaya/complingua/blob/main/%D0%9F%D0%B5%D1%82%D1%80%D0%BE%D0%B2%D1%81%D0%BA%D0%B0%D1%8F_%22tokenization_tutorial_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d60a714e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d60a714e",
        "outputId": "18f2f811-2a21-4ed4-9fe8-adb59e59dc9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligencethat', 'gives', 'computers', 'the', 'ability', 'to', 'understand', 'text', 'and', 'spoken', 'words', 'in', 'much', 'the', 'same', 'way', 'human', 'beings', 'can', '.']\n",
            "['Natural language processing (NLP) is a field of artificial intelligencethat gives computers the ability to understand text and spoken words in much the same way human beings can.']\n",
            "NLTK: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligencethat', 'gives', 'computers', 'the', 'ability', 'to', 'understand', 'text', 'and', 'spoken', 'words', 'in', 'much', 'the', 'same', 'way', 'human', 'beings', 'can', '.']\n",
            "spaCy: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligencethat', 'gives', 'computers', 'the', 'ability', 'to', 'understand', 'text', 'and', 'spoken', 'words', 'in', 'much', 'the', 'same', 'way', 'human', 'beings', 'can', '.']\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Gensim: ['natural', 'language', 'processing', 'nlp', 'is', 'field', 'of', 'artificial', 'gives', 'computers', 'the', 'ability', 'to', 'understand', 'text', 'and', 'spoken', 'words', 'in', 'much', 'the', 'same', 'way', 'human', 'beings', 'can']\n"
          ]
        }
      ],
      "source": [
        "article = 'Natural language processing (NLP) is a field of artificial intelligence\\\n",
        "that gives computers the ability to understand text and spoken words in much the same way human beings can.'\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "article = \"Natural language processing (NLP) is a field of artificial intelligence\\\n",
        "that gives computers the ability to understand text and spoken words in much the same way human beings can.\"\n",
        "print(word_tokenize(article))\n",
        "print(sent_tokenize(article))\n",
        "print(\"NLTK:\", word_tokenize(article))\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Natural language processing (NLP) is a field of artificial intelligence\\\n",
        "that gives computers the ability to understand text and spoken words in much the same way human beings can.\")\n",
        "[t.text for t in doc]\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(article)\n",
        "print(\"spaCy:\", [t.text for t in doc])\n",
        "\n",
        "\n",
        "\n",
        "!pip install gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "article = \"Natural language processing (NLP) is a field of artificial intelligence\\\n",
        "that gives computers the ability to understand text and spoken words in much the same way human beings can.\"\n",
        "simple_preprocess(article)\n",
        "print(\"Gensim:\", simple_preprocess(article))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e575f14e",
      "metadata": {
        "id": "e575f14e"
      },
      "source": [
        "\n",
        "## 7. Задача для самостоятельного разбора  \n",
        "\n",
        "Прочитать статью:  \n",
        "**\"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models\"**  \n",
        "\n",
        "### Вопросы для обсуждения:\n",
        "1. Какие преимущества даёт гибридная токенизация?  \n",
        "2. Какие проблемы возникают при работе с открытым словарём?  \n",
        "3. В каких NLP-задачах вы бы применили гибридную токенизацию?  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Гибридная модель совмещает обработку слов и символов (character-level), она не производит unk-токены (unknow) на выходе, в отличие от только словесных моделей. Поэтому может переводить сложные, редкие слова, имена и многосоставные термины через \"символьные\" модели. Такая модель быстрее обучается и тратит меньше ресурсов на вычисления: «Our hybrid NMT offers a twofold advantage: it is much faster and easier to train than character-based models; at the same time, it never produces unknown words as in the case of word-based ones» На тесте гибридная модель показала улучшение по сравнению с моделями, использующими замену unk. Также эти символьные компоненты показывают? слова на основе их букв, это необходимо для языков с выраженной морфологией (например, чешского).\n",
        "\n",
        "# В  словесных моделях редкие и морфологически сложные слова заменяются на unk, много смысла и информации теряется. Открытые словари хорошо обучаются именно «частым словам. Особенно для различия между языками, транслитерация и т.д. нужна более сложная система, а постобработка unk-токенов через таблицы соответствий не включает в себя морфологию.\n",
        "#Я бы применила гибридную токенизацию в задачах, связанных с машинным переводом. Это бы как раз облегчило перевод языков с развитой морфологией, помогло бы с переводом редких слов, имен, терминов, особенно в переводе современных текстов. Или, например, при обработке \"устной речи\"."
      ],
      "metadata": {
        "id": "AHr19B1k7gXb"
      },
      "id": "AHr19B1k7gXb",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YzWDwoBVeu6b"
      },
      "id": "YzWDwoBVeu6b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}